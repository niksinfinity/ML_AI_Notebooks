{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mnist Experiment Using Neural Network",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niksinfinity/ML_AI_Notebooks/blob/master/Mnist_Experiment_Using_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbp-9nfgHVG5",
        "colab_type": "code",
        "outputId": "a20e842c-6a5a-44fb-9eb0-1e69fbdf8218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os \n",
        "import h5py\n",
        "os.listdir('./')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'ML_datasets', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I38kr1qk_Q3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!git clone https://github.com/niksinfinity/ML_datasets.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07cMcX-4AN_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = h5py.File('./ML_datasets/catsvsdogs/train_catvnoncat.h5')\n",
        "test_data = h5py.File('./ML_datasets/catsvsdogs/test_catvnoncat.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BAUbtbZAQLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes_train,train_x_orig,train_y_orig=map(lambda x:np.array(x),train_data.values())\n",
        "classes_test,test_x_orig,test_y_orig=map(lambda x:np.array(x),test_data.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktbwg90_A4jD",
        "colab_type": "code",
        "outputId": "0f5f9f7b-db18-4081-88ec-2edc6d62c6b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "train_y = train_y_orig.reshape(train_y_orig.shape[0],1).T\n",
        "test_y =  test_y_orig.reshape(test_y_orig.shape[0],1).T\n",
        "print(train_x_flatten.shape)\n",
        "print(test_x_flatten.shape)\n",
        "print(test_y.shape)\n",
        "print(train_y.shape)\n",
        "train_x = train_x_flatten /255\n",
        "test_x = test_x_flatten /255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12288, 209)\n",
            "(12288, 50)\n",
            "(1, 50)\n",
            "(1, 209)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGOzd8D-xgth",
        "colab_type": "text"
      },
      "source": [
        "Deep Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQOrgK0HhBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z)),z\n",
        "\n",
        "def relu(z):\n",
        "  return np.maximum(0,z),z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbIuqC6sIHDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6nRfmDrLs9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA,Z):\n",
        "\n",
        "  #dZ[Z <= 0] = 0\n",
        "  dZ = np.array(dA, copy=True)\n",
        "  dZ[Z <= 0] = 0\n",
        "  \n",
        "  return dZ\n",
        "\n",
        "def sigmoid_backward(dA,Z):\n",
        "  Z = sigmoid(Z)\n",
        "  Z = Z[0] * (1-Z[0])\n",
        "  \n",
        "  return dA*Z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnqHkLmRBZfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-as4UAdxnGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6JqiEtZxs96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    np.random.seed(3)\n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "#         if dropout:\n",
        "#           DL = np.random.randn(A.shape[0],A.shape[1]) < keepdims\n",
        "#           A = np.multiply(A,DL)\n",
        "#           A = A / keepdims\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A, activation_cache = relu(Z)\n",
        "        if dropout:\n",
        "          DL = np.random.rand(A.shape[0],A.shape[1]) <= keepdims\n",
        "          A = np.multiply(A,DL)\n",
        "          A = A / keepdims\n",
        "    \n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nG65JrFx8FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A,activation='relu',b=parameters['b'+str(l)],W=parameters['W'+str(l)])\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A,activation='sigmoid',b=parameters['b'+str(L)],W=parameters['W'+str(L)])\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GCNGpmLyMot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = -(np.sum(np.multiply(Y,np.log(AL)) + np.multiply((1-Y),np.log(1-AL))))/m\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CatqwTv2yZIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = np.dot(dZ,A_prev.T)/m\n",
        "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCXwxmjhyhAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the relu_backwardprevious layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        \n",
        "        dZ = relu_backward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(cache=linear_cache,dZ=dZ)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "       \n",
        "        dZ = sigmoid_backward(dA,activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(cache=linear_cache,dZ=dZ)\n",
        "    \n",
        "   \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BnpAp0aytKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(activation='sigmoid',cache=current_cache,dA=dAL)\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(activation='relu',cache=current_cache,dA=grads[\"dA\"+str(l+1)])\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm_H7zoyy5LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter.\n",
        "    for l in range(L):\n",
        "       \n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads['dW'+ str(l+1)])\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads['db'+ str(l+1)])\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qYApSD7zojS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJkI1ptiz7f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.000075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. (â‰ˆ 1 line of code)\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, caches = L_model_forward(parameters=parameters,X=X)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL,Y)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL=AL,caches=caches,Y=Y)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(grads=grads,learning_rate=learning_rate,parameters=parameters)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM_q0Y1J0WUG",
        "colab_type": "code",
        "outputId": "652a5915-a2fc-4803-937c-938715616489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3729
        }
      },
      "source": [
        "\n",
        "layers_dims = [12288,12,16,10,1] #  4-layer model\n",
        "keepdims = 0.905\n",
        "dropout = True\n",
        "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 20000,learning_rate=0.00099, print_cost = True)\n",
        "print(\"Train Accuracy -->{}\".format(predict(train_x,train_y,parameters)[2]))\n",
        "dropout=False\n",
        "print(\"Testing Accuracy --->{}\".format(predict(test_x,test_y,parameters)[2]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.717528\n",
            "Cost after iteration 100: 0.626225\n",
            "Cost after iteration 200: 0.604746\n",
            "Cost after iteration 300: 0.587905\n",
            "Cost after iteration 400: 0.573506\n",
            "Cost after iteration 500: 0.558693\n",
            "Cost after iteration 600: 0.545248\n",
            "Cost after iteration 700: 0.532457\n",
            "Cost after iteration 800: 0.520504\n",
            "Cost after iteration 900: 0.507770\n",
            "Cost after iteration 1000: 0.494612\n",
            "Cost after iteration 1100: 0.479827\n",
            "Cost after iteration 1200: 0.464287\n",
            "Cost after iteration 1300: 0.448244\n",
            "Cost after iteration 1400: 0.432522\n",
            "Cost after iteration 1500: 0.417418\n",
            "Cost after iteration 1600: 0.402583\n",
            "Cost after iteration 1700: 0.387919\n",
            "Cost after iteration 1800: 0.373808\n",
            "Cost after iteration 1900: 0.360517\n",
            "Cost after iteration 2000: 0.347507\n",
            "Cost after iteration 2100: 0.335766\n",
            "Cost after iteration 2200: 0.323932\n",
            "Cost after iteration 2300: 0.312270\n",
            "Cost after iteration 2400: 0.300984\n",
            "Cost after iteration 2500: 0.290887\n",
            "Cost after iteration 2600: 0.280597\n",
            "Cost after iteration 2700: 0.270208\n",
            "Cost after iteration 2800: 0.260923\n",
            "Cost after iteration 2900: 0.252430\n",
            "Cost after iteration 3000: 0.244458\n",
            "Cost after iteration 3100: 0.236762\n",
            "Cost after iteration 3200: 0.229239\n",
            "Cost after iteration 3300: 0.221729\n",
            "Cost after iteration 3400: 0.214841\n",
            "Cost after iteration 3500: 0.208091\n",
            "Cost after iteration 3600: 0.202665\n",
            "Cost after iteration 3700: 0.196541\n",
            "Cost after iteration 3800: 0.190913\n",
            "Cost after iteration 3900: 0.185915\n",
            "Cost after iteration 4000: 0.180681\n",
            "Cost after iteration 4100: 0.175234\n",
            "Cost after iteration 4200: 0.170077\n",
            "Cost after iteration 4300: 0.164698\n",
            "Cost after iteration 4400: 0.159344\n",
            "Cost after iteration 4500: 0.154061\n",
            "Cost after iteration 4600: 0.148807\n",
            "Cost after iteration 4700: 0.143699\n",
            "Cost after iteration 4800: 0.139109\n",
            "Cost after iteration 4900: 0.134519\n",
            "Cost after iteration 5000: 0.130552\n",
            "Cost after iteration 5100: 0.126951\n",
            "Cost after iteration 5200: 0.123196\n",
            "Cost after iteration 5300: 0.120001\n",
            "Cost after iteration 5400: 0.116527\n",
            "Cost after iteration 5500: 0.113021\n",
            "Cost after iteration 5600: 0.109487\n",
            "Cost after iteration 5700: 0.106084\n",
            "Cost after iteration 5800: 0.102782\n",
            "Cost after iteration 5900: 0.099635\n",
            "Cost after iteration 6000: 0.096549\n",
            "Cost after iteration 6100: 0.093661\n",
            "Cost after iteration 6200: 0.091056\n",
            "Cost after iteration 6300: 0.088085\n",
            "Cost after iteration 6400: 0.085398\n",
            "Cost after iteration 6500: 0.082763\n",
            "Cost after iteration 6600: 0.079989\n",
            "Cost after iteration 6700: 0.077362\n",
            "Cost after iteration 6800: 0.074905\n",
            "Cost after iteration 6900: 0.072423\n",
            "Cost after iteration 7000: 0.069919\n",
            "Cost after iteration 7100: 0.067702\n",
            "Cost after iteration 7200: 0.065561\n",
            "Cost after iteration 7300: 0.063435\n",
            "Cost after iteration 7400: 0.061413\n",
            "Cost after iteration 7500: 0.059520\n",
            "Cost after iteration 7600: 0.057614\n",
            "Cost after iteration 7700: 0.055974\n",
            "Cost after iteration 7800: 0.054354\n",
            "Cost after iteration 7900: 0.052740\n",
            "Cost after iteration 8000: 0.051316\n",
            "Cost after iteration 8100: 0.049891\n",
            "Cost after iteration 8200: 0.048547\n",
            "Cost after iteration 8300: 0.047245\n",
            "Cost after iteration 8400: 0.045987\n",
            "Cost after iteration 8500: 0.044796\n",
            "Cost after iteration 8600: 0.043596\n",
            "Cost after iteration 8700: 0.042550\n",
            "Cost after iteration 8800: 0.041438\n",
            "Cost after iteration 8900: 0.040297\n",
            "Cost after iteration 9000: 0.039229\n",
            "Cost after iteration 9100: 0.038283\n",
            "Cost after iteration 9200: 0.037407\n",
            "Cost after iteration 9300: 0.036244\n",
            "Cost after iteration 9400: 0.035095\n",
            "Cost after iteration 9500: 0.033995\n",
            "Cost after iteration 9600: 0.032989\n",
            "Cost after iteration 9700: 0.031978\n",
            "Cost after iteration 9800: 0.031059\n",
            "Cost after iteration 9900: 0.030230\n",
            "Cost after iteration 10000: 0.029377\n",
            "Cost after iteration 10100: 0.028568\n",
            "Cost after iteration 10200: 0.027716\n",
            "Cost after iteration 10300: 0.026895\n",
            "Cost after iteration 10400: 0.026086\n",
            "Cost after iteration 10500: 0.025115\n",
            "Cost after iteration 10600: 0.024279\n",
            "Cost after iteration 10700: 0.023494\n",
            "Cost after iteration 10800: 0.022720\n",
            "Cost after iteration 10900: 0.022023\n",
            "Cost after iteration 11000: 0.021337\n",
            "Cost after iteration 11100: 0.020727\n",
            "Cost after iteration 11200: 0.020129\n",
            "Cost after iteration 11300: 0.019580\n",
            "Cost after iteration 11400: 0.019036\n",
            "Cost after iteration 11500: 0.018563\n",
            "Cost after iteration 11600: 0.018092\n",
            "Cost after iteration 11700: 0.017688\n",
            "Cost after iteration 11800: 0.017274\n",
            "Cost after iteration 11900: 0.016888\n",
            "Cost after iteration 12000: 0.016501\n",
            "Cost after iteration 12100: 0.016149\n",
            "Cost after iteration 12200: 0.015835\n",
            "Cost after iteration 12300: 0.015496\n",
            "Cost after iteration 12400: 0.015170\n",
            "Cost after iteration 12500: 0.014835\n",
            "Cost after iteration 12600: 0.014524\n",
            "Cost after iteration 12700: 0.014227\n",
            "Cost after iteration 12800: 0.013953\n",
            "Cost after iteration 12900: 0.013673\n",
            "Cost after iteration 13000: 0.013422\n",
            "Cost after iteration 13100: 0.013170\n",
            "Cost after iteration 13200: 0.012949\n",
            "Cost after iteration 13300: 0.012702\n",
            "Cost after iteration 13400: 0.012480\n",
            "Cost after iteration 13500: 0.012259\n",
            "Cost after iteration 13600: 0.012056\n",
            "Cost after iteration 13700: 0.011853\n",
            "Cost after iteration 13800: 0.011662\n",
            "Cost after iteration 13900: 0.011467\n",
            "Cost after iteration 14000: 0.011283\n",
            "Cost after iteration 14100: 0.011114\n",
            "Cost after iteration 14200: 0.010950\n",
            "Cost after iteration 14300: 0.010790\n",
            "Cost after iteration 14400: 0.010642\n",
            "Cost after iteration 14500: 0.010479\n",
            "Cost after iteration 14600: 0.010329\n",
            "Cost after iteration 14700: 0.010193\n",
            "Cost after iteration 14800: 0.010047\n",
            "Cost after iteration 14900: 0.009902\n",
            "Cost after iteration 15000: 0.009762\n",
            "Cost after iteration 15100: 0.009624\n",
            "Cost after iteration 15200: 0.009494\n",
            "Cost after iteration 15300: 0.009364\n",
            "Cost after iteration 15400: 0.009238\n",
            "Cost after iteration 15500: 0.009123\n",
            "Cost after iteration 15600: 0.009026\n",
            "Cost after iteration 15700: 0.008921\n",
            "Cost after iteration 15800: 0.008819\n",
            "Cost after iteration 15900: 0.008720\n",
            "Cost after iteration 16000: 0.008613\n",
            "Cost after iteration 16100: 0.008510\n",
            "Cost after iteration 16200: 0.008412\n",
            "Cost after iteration 16300: 0.008303\n",
            "Cost after iteration 16400: 0.008201\n",
            "Cost after iteration 16500: 0.008104\n",
            "Cost after iteration 16600: 0.008006\n",
            "Cost after iteration 16700: 0.007907\n",
            "Cost after iteration 16800: 0.007813\n",
            "Cost after iteration 16900: 0.007712\n",
            "Cost after iteration 17000: 0.007622\n",
            "Cost after iteration 17100: 0.007521\n",
            "Cost after iteration 17200: 0.007429\n",
            "Cost after iteration 17300: 0.007336\n",
            "Cost after iteration 17400: 0.007251\n",
            "Cost after iteration 17500: 0.007160\n",
            "Cost after iteration 17600: 0.007078\n",
            "Cost after iteration 17700: 0.006990\n",
            "Cost after iteration 17800: 0.006910\n",
            "Cost after iteration 17900: 0.006827\n",
            "Cost after iteration 18000: 0.006752\n",
            "Cost after iteration 18100: 0.006670\n",
            "Cost after iteration 18200: 0.006595\n",
            "Cost after iteration 18300: 0.006527\n",
            "Cost after iteration 18400: 0.006453\n",
            "Cost after iteration 18500: 0.006377\n",
            "Cost after iteration 18600: 0.006295\n",
            "Cost after iteration 18700: 0.006215\n",
            "Cost after iteration 18800: 0.006131\n",
            "Cost after iteration 18900: 0.006042\n",
            "Cost after iteration 19000: 0.005960\n",
            "Cost after iteration 19100: 0.005872\n",
            "Cost after iteration 19200: 0.005784\n",
            "Cost after iteration 19300: 0.005697\n",
            "Cost after iteration 19400: 0.005610\n",
            "Cost after iteration 19500: 0.005529\n",
            "Cost after iteration 19600: 0.005452\n",
            "Cost after iteration 19700: 0.005376\n",
            "Cost after iteration 19800: 0.005298\n",
            "Cost after iteration 19900: 0.005227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcHHWd//HXp3uuHDOTawjJTG4S\nIYQEwpBwExeUhMVELkkEFTwQlPXAXRdXf8riT38ii66ruByKgAuGQ2QDAhGRQ5RAJiEJJJCDkPtg\nck+OyVyf3x9VM3SGmckkmZrqnn4/H496TNe3vlX96eqe/vS3vlXfMndHREQEIBF3ACIikj6UFERE\npImSgoiINFFSEBGRJkoKIiLSRElBRESaKClIRjKzp83sM3HHIdLVKCnIITGzVWZ2XtxxuPsUd78v\n7jgAzOwFM/t8JzxPvpndY2a7zGyTmd1wkPpfD+vtCtfLT1k21MyeN7O9ZvZ28/f0IOuebmavmVmV\nmS0yszNTlpmZfdvM1oTrzjSzoo7cDxItJQVJO2aWE3cMjdIpFuAmYCQwBPgw8E0zm9xSRTM7H7gR\nODesPxz495QqvwNeB/oC3wYeNbOSg61rZn2AJ4BbgV7Aj4EnzKx3uN1PA58CzgAGAt2Anx/xK5fO\n4+6aNLV7AlYB57Wy7EJgAbAD+DswNmXZjcA7QBWwBLgoZdlVwN+AnwJbgf8blr0M/AewHXgXmJKy\nzgvA51PWb6vuMOCl8Ln/DNwO/E8rr2ESsA74V2AT8FugN/AkUBlu/0mgLKz/A6AeqAZ2A78Iy48F\nngW2AUuBT3TAvt8AfDRl/vvAzFbqPgj8MGX+XGBT+HgUsB8oTFn+V+Dadqx7IbC42XMtAz4XPn4U\n+JeUZaeH+6Z73J9dTe2b1FKQDmFmJwH3AF8k+PV5JzAr5bDDO8BZQDHBr87/MbMBKZuYCKwE+hN8\n0TaWLQX6Efwi/bWZWSshtFX3QeC1MK6bCH7JtuVooA/Br+RrCFrUvwnnBwP7gF8AuPu3Cb5Qr3f3\nnu5+vZn1IEgIDwJHAdOBX5rZ6JaezMx+aWY7WpkWhXV6AwOAhSmrLgSOb+U1HN9C3f5m1jdcttLd\nq1rZVlvrAjR/DwwY02w+9XE+QQtHMoCSgnSUa4A73f1Vd6/34Hj/fuBUAHd/xN03uHuDuz8ELAcm\npKy/wd1/7u517r4vLFvt7ne7ez1wH8GXYv9Wnr/FumY2GDgF+K6717j7y8Csg7yWBuB77r7f3fe5\n+1Z3/7277w2/SH8AnNPG+hcCq9z9N+HreR34PXBZS5Xd/Uvu3quVaWxYrWf4d2fKqjuBwlZi6NlC\nXcL6zZc131Zb674CDDSzGWaWG3b2jwC6h3WeAT4f9lkUE7S4SFkuaU5JQTrKEOAbqb9ygUEEx5Ux\ns0+b2YKUZWMIftU3WtvCNjc1PnD3veHDni3Ua6vuQGBbSllrz5Wq0t2rG2fMrLuZ3Wlmq81sF8Gh\nqF5mlmxl/SHAxGb74gqCFsjh2h3+Te20LSI4JNZa/eZ1Ces3X9Z8W62u6+5bgWnADcBmYDLBIbl1\nYZ17CPorXgAWA8+H5Y3LJc0pKUhHWQv8oNmv3O7u/jszGwLcDVwP9HX3XsCbHHiYIarhejcCfcws\n9ZfqoIOs0zyWbwAfAia6exFwdlhurdRfC7zYbF/0dPfrWnoyM7vDzHa3Mi0GcPft4WsZl7LqOIIv\n3pYsbqHu5vBLfTEw3MwKmy1f3I51cfcX3f0Ud+9DcCjuWILDc4Qtwe+5+1B3Lwu3tT6cJAMoKcjh\nyDWzgpQph+BL/1ozmxieltjDzP4x/OLpQfDFWQlgZldz4DHoyLj7aqACuMnM8szsNOBjh7iZQoJ+\nhB3h2Tffa7Z8M8EZOo2eBEaZ2afCQyy5ZnaKmR3XSozXhkmjpSm1z+B+4Dtm1tvMjgW+ANzbSsz3\nA58zs9Fm1gv4TmNdd19GcELA98L37yJgLMEhrjbXhaD/KHxNRQSd+2vdfXa4rI+ZjQg/A6OBnwA3\nu3tDK3FKmlFSkMPxFMGXZON0k7tXEHxJ/YLgDJ0VBGcF4e5LgNsIjkdvBk4gONuos1wBnMb7ZzY9\nRNDf0V7/SXBq5RZgDsFx81Q/Ay41s+1m9l9hv8NHCTqYNxAc2rqFoMP1SHyPoMN+NfAicKu7PwNg\nZoPDlsVggLD8xwSHb9aE66Qms+lAOcF79SPgUnevbOe63wz3xVqCvpuLUpb1I/h87AGeBu5x97uO\n8HVLJzJ33WRHsouZPQS87e7Nf/GLZD21FKTLCw/djDCzRHix1zTg8bjjEklH6XS1pkhUjgYeI7hO\nYR1wXXiaqIg0o8NHIiLSRIePRESkScYdPurXr58PHTo07jBERDLKvHnztrh7ycHqZVxSGDp0KBUV\nFXGHISKSUcxsdXvq6fCRiIg0UVIQEZEmSgoiItJESUFERJooKYiISBMlBRERaaKkICIiTbImKcxd\ntY0fP/M2DQ0a1kNEpDVZkxQWrt3BL194h901dXGHIiKStrImKRQV5AJQVa2kICLSmkiTgplNNrOl\nZrbCzG5sYflPw5u5LzCzZeENziNRWBCM6LFrX21UTyEikvEiG/vIzJLA7cBHCMawn2tms8JbMwLg\n7l9Pqf9PwElRxVOoloKIyEFF2VKYAKxw95XuXgPMJLjjVWtmAL+LKpiibmopiIgcTJRJoZTgxt6N\n1oVlH2BmQ4BhwF9aWX6NmVWYWUVlZeVhBdPUUtivpCAi0pp06WieDjzq7vUtLXT3u9y93N3LS0oO\nOhx4i4qa+hR0+EhEpDVRJoX1wKCU+bKwrCXTifDQEaT2KailICLSmiiTwlxgpJkNM7M8gi/+Wc0r\nmdmxQG/glQhjIS8nQUFugl3qaBYRaVVkScHd64DrgdnAW8DD7r7YzG42s6kpVacDM9098kuNCwty\n1VIQEWlDpLfjdPengKealX232fxNUcaQqqggR30KIiJtSJeO5k5RWJDLLrUURERalWVJIUcXr4mI\ntCGrkkJRN7UURETakl1JQS0FEZE2ZVlSyNUwFyIibciqpFBYkMP+ugb217V44bSISNbLqqRQ1E0j\npYqItCWrkkLjPRWUFEREWpZVSaHx7mvqVxARaVlWJQXdaEdEpG1ZlhTC4bN1rYKISIuyKim839Gs\npCAi0pKsSgrqaBYRaVtWJYWeeTmYqaNZRKQ1WZUUEgmjuFsulbtr4g5FRCQtZVVSABhb1ouKVdvi\nDkNEJC1lXVI4fURflr+3m8qq/XGHIiKSdrIyKQC8snJrzJGIiKSfrEsKxw8sprAgh1fe2RJ3KCIi\naSfSpGBmk81sqZmtMLMbW6nzCTNbYmaLzezBKOMBSCaMicP68vd31FIQEWkusqRgZkngdmAKMBqY\nYWajm9UZCXwLOMPdjwe+FlU8qU4f0ZfVW/eyZuvezng6EZGMEWVLYQKwwt1XunsNMBOY1qzOF4Db\n3X07gLu/F2E8TT4yuj8Azyze2BlPJyKSMaJMCqXA2pT5dWFZqlHAKDP7m5nNMbPJLW3IzK4xswoz\nq6isrDziwAb16c4JpcU89camI96WiEhXEndHcw4wEpgEzADuNrNezSu5+13uXu7u5SUlJR3yxJPH\nHM2CtTtYv2Nfh2xPRKQriDIprAcGpcyXhWWp1gGz3L3W3d8FlhEkichNGXM0AM+8qdaCiEijKJPC\nXGCkmQ0zszxgOjCrWZ3HCVoJmFk/gsNJKyOMqcnwkp6MHlDE4683z1MiItkrsqTg7nXA9cBs4C3g\nYXdfbGY3m9nUsNpsYKuZLQGeB/7F3TvtXNHLyst4Y/1OlmzY1VlPKSKS1szd447hkJSXl3tFRUWH\nbGvH3hom/PA5PjlhMDdNPb5Dtikiko7MbJ67lx+sXtwdzbHq1T2P848/mj+8vp7q2vq4wxERiV1W\nJwWAy8sHsXNfLbMXq8NZRCTrk8LpI/pS1rsbD1esPXhlEZEuLuuTQiJhfKJ8EH9bsZW12zTshYhk\nt6xPCgCXnlyGGTyi1oKIZDklBWBgr26cPbKER+ato74hs87GEhHpSEoKoctPGcTGndW8tPzIx1YS\nEclUSgqh847rT58eeTw8V4eQRCR7KSmE8nISXHxSKX9+azNbduv+zSKSnZQUUkyfMJjaemfma2vi\nDkVEJBZKCimOOaonZ48q4f5XVlNT1xB3OCIinU5JoZmrzxjKe1X7efpN3ZVNRLKPkkIz54wsYXi/\nHtzz8rtk2mCBIiJHSkmhmUTCuPqMoSxct5P5a3bEHY6ISKdSUmjBxePLKCzI4Td/ezfuUEREOpWS\nQgt65OcwY8Jgnn5zExt0D2cRySJKCq349GlDcHd+O2d13KGIiHQaJYVWlPXuzvnHH82Dr65hX41u\nwCMi2SHSpGBmk81sqZmtMLMbW1h+lZlVmtmCcPp8lPEcqqvPGMbOfbX84fX1cYciItIpIksKZpYE\nbgemAKOBGWY2uoWqD7n7ieH0q6jiORynDO3NmNIifvM3nZ4qItkhypbCBGCFu6909xpgJjAtwufr\ncGbG1acPY/l7u3l5xZa4wxERiVyUSaEUSB1ydF1Y1twlZrbIzB41s0ERxnNYLhw3gH4987nnZZ2e\nKiJdX9wdzU8AQ919LPAscF9LlczsGjOrMLOKysrOvd9Bfk6SK08dzPNLK1lZubtTn1tEpLNFmRTW\nA6m//MvCsibuvtXdG8ep/hVwcksbcve73L3c3ctLSkoiCbYtn5w4mGTCeLhiXac/t4hIZ4oyKcwF\nRprZMDPLA6YDs1IrmNmAlNmpwFsRxnPYjios4OyR/fjfBetp0O06RaQLiywpuHsdcD0wm+DL/mF3\nX2xmN5vZ1LDaV8xssZktBL4CXBVVPEfqovFlbNxZzZyVW+MORUQkMjlRbtzdnwKealb23ZTH3wK+\nFWUMHeWjo/vTMz+Hx15fz+nH9Is7HBGRSMTd0ZwxCnKTTBlzNE+/sVFXOItIl6WkcAguGl/Knpp6\n/rRkU9yhiIhEQknhEJw6rC8Diws07IWIdFlKCocgkTA+flIpf12+hcqq/QdfQUQkwygpHKKLx5dS\n3+A8sXBD3KGIiHQ4JYVDdMxRhYweUMQsJQUR6YKUFA7Dx8YNZMHaHazZujfuUEREOpSSwmH42Ljg\nQuwnFqm1ICJdi5LCYSjr3Z2Th/RWv4KIdDlKCodp6riBvL2pimWbq+IORUSkwygpHKYLThhAwmDW\nArUWRKTrUFI4TCWF+Zw+oh9PLNqgW3WKSJehpHAEpo4byOqte1m0bmfcoYiIdAglhSNw/pijyUsm\nNOyFiHQZSgpHoLhbLueNPopZCzdQW98QdzgiIkdMSeEIXTK+jG17anhhaefeO1pEJApKCkfo7FEl\n9OuZx+/n6f7NIpL5lBSOUG4ywbQTS3nu7c1s31MTdzgiIkdESaEDXDK+jNp617AXIpLxIk0KZjbZ\nzJaa2Qozu7GNepeYmZtZeZTxRGX0wCKOG1CkQ0gikvEiSwpmlgRuB6YAo4EZZja6hXqFwFeBV6OK\npTNcMr6Uhet2suI9DXshIpkrypbCBGCFu6909xpgJjCthXrfB24BqiOMJXLTTiwlmTAenadrFkQk\nc0WZFEqBtSnz68KyJmY2Hhjk7n9sa0Nmdo2ZVZhZRWVlep76WVKYzzmjSnj89fXUN2jYCxHJTLF1\nNJtZAvgJ8I2D1XX3u9y93N3LS0pKog/uMF0yvoxNu6r5+ztb4g5FROSwRJkU1gODUubLwrJGhcAY\n4AUzWwWcCszK1M5mgHOPO4qighx1OItIxmpXUjCzy9pT1sxcYKSZDTOzPGA6MKtxobvvdPd+7j7U\n3YcCc4Cp7l7R7ujTTEFuko+NG8gzizexq7o27nBERA5Ze1sK32pnWRN3rwOuB2YDbwEPu/tiM7vZ\nzKYeWpiZ4/JTBlFd28DjGiRPRDJQTlsLzWwKcAFQamb/lbKoCKg72Mbd/SngqWZl322l7qSDbS8T\njC3rxZjSIh58dQ2fOnUIZhZ3SCIi7XawlsIGoILgdNF5KdMs4PxoQ8tcn5wwhLc3VTF/zY64QxER\nOSRtJgV3X+ju9wHHuPt94eNZBNcfbO+UCDPQ1BMH0iMvyYOvrok7FBGRQ9LePoVnzazIzPoA84G7\nzeynEcaV0Xrm5zDtpFKeXLSBnXvV4SwimaO9SaHY3XcBFwP3u/tE4Nzowsp8n5wwmP11DTz2uk5P\nFZHM0d6kkGNmA4BPAE9GGE+XMaa0mHFlxTz46hrcdYWziGSG9iaFmwlOLX3H3eea2XBgeXRhdQ1X\nTBzC8vd28+q72+IORUSkXdqVFNz9EXcf6+7XhfMr3f2SaEPLfB8bN5Dibrnc/8qquEMREWmX9l7R\nXGZmfzCz98Lp92ZWFnVwma5bXpLppwxi9uLNbNy5L+5wREQOqr2Hj35DcCrqwHB6IiyTg7jy1CE0\nuPM/c1bHHYqIyEG1NymUuPtv3L0unO4F0ne40jQyqE93zh99NL99ZTW79x/0InARkVi1NylsNbMr\nzSwZTlcCW6MMrCu5btIIdlXX8YBaCyKS5tqbFD5LcDrqJmAjcClwVUQxdTnjBvXizGP68auX36W6\ntj7ucEREWnUop6R+xt1L3P0ogiTx79GF1fV8adIIKqv28/v5uphNRNJXe5PC2NSxjtx9G3BSNCF1\nTaeN6Mu4Qb2448V3qKtviDscEZEWtTcpJMysd+NMOAZSm8Nuy4HMjC9PGsHabfv44xsb4w5HRKRF\n7U0KtwGvmNn3zez7wN+BH0cXVtd03nH9GXlUT/77hXc09IWIpKX2XtF8P8FgeJvD6WJ3/22UgXVF\niYRx3aQRvL2pir+8/V7c4YiIfEB7Wwq4+xJ3/0U4LYkyqK7sY+MGUta7G7c/v0KtBRFJO+1OCtIx\ncpMJvnj2cOav2aGB8kQk7USaFMxsspktNbMVZnZjC8uvNbM3zGyBmb1sZqOjjCddXFY+iH498/jl\nC+/EHYqIyAEiSwpmlgRuB6YAo4EZLXzpP+juJ7j7iQQd1z+JKp50UpCb5LNnDuOlZZW8sW5n3OGI\niDSJsqUwgeBezivdvQaYCUxLrRDeza1RDyBrDrJfeeoQCgty+MXzui2FiKSPKJNCKbA2ZX5dWHYA\nM/uymb1D0FL4SksbMrNrzKzCzCoqKysjCbazFRXk8vkzhzN78Wbmrd5+8BVERDpB7B3N7n67u48A\n/hX4Tit17nL3cncvLynpOoOzfv6sYZQU5vPDp97SmUgikhaiTArrgUEp82VhWWtmAh+PMJ600yM/\nhxs+Mop5q7cze/GmuMMREYk0KcwFRprZMDPLA6YT3KiniZmNTJn9R7Lwvs+XnVzGyKN6csszS6nV\nmEgiErPIkoK71wHXA7OBt4CH3X2xmd1sZlPDateb2WIzWwDcAHwmqnjSVU4ywbcuOJZ3t+zhd6+t\niTscEclylmnHssvLy72ioiLuMDqUuzPj7jks37ybF/5lEoUFuXGHJCJdjJnNc/fyg9WLvaNZghFU\nv33BaLbuqeGOF3VBm4jER0khTZxQVsy0Ewfyq7++y8ad++IOR0SylJJCGvnnj34Id7jtT8viDkVE\nspSSQhoZ1Kc7V50xlN/PX8eSDbsOvoKISAdTUkgzX550DEUFubqgTURioaSQZoq75/KNj47i5RVb\neKRiXdzhiEiWUVJIQ1dOHMLEYX34/pNL2LBDnc4i0nmUFNJQImHceuk46t258bE3dBhJRDqNkkKa\nGty3OzdOOZaXllXy0Ny1B19BRKQDKCmksSsnDuG04X35/pNLWLVlT9zhiEgWUFJIY4mE8R+fGEdO\nMsFXZr5OTZ0GzBORaCkppLnSXt245ZITWLRuJ7f9aWnc4YhIF6ekkAEmjxnAFRMHc+dLK3lpWde4\n85yIpCclhQzxfy4czaj+Pbnh4YVUVu2POxwR6aKUFDJEQW6Sn88YT1V1Lf/8yEIaGnSaqoh0PCWF\nDPKhowv5PxeO5sVlldz50sq4wxGRLkhJIcNcMXEwF44dwK2z3+ZF9S+ISAdTUsgwZsaPLx3Lh44u\n4p8enM+7un5BRDqQkkIG6p6Xw12fOplkwvjC/RVUVdfGHZKIdBGRJgUzm2xmS81shZnd2MLyG8xs\niZktMrPnzGxIlPF0JYP6dOf2K8bz7pY9fPG389hXUx93SCLSBUSWFMwsCdwOTAFGAzPMbHSzaq8D\n5e4+FngU+HFU8XRFp4/ox62XjmXOyq1cfe9rSgwicsSibClMAFa4+0p3rwFmAtNSK7j78+6+N5yd\nA5RFGE+XdPH4Mn56+Ym8+u42rntgnobCEJEjEmVSKAVSh/dcF5a15nPA0y0tMLNrzKzCzCoqK3XG\nTXPTTizl/110Ai8sreTrDy+gXtcwiMhhyok7AAAzuxIoB85pabm73wXcBVBeXq5vvBZMnzCYquo6\nfvDUWxTm5/DDi04gkbC4wxKRDBNlUlgPDEqZLwvLDmBm5wHfBs5xd43fcAS+cPZwdlXX8vO/rKC2\n3rnlkhPISeoEMxFpvyiTwlxgpJkNI0gG04FPplYws5OAO4HJ7v5ehLFkjRs+MoqcRIKf/nlZkCBm\nnERBbjLusEQkQ0T2M9Ld64DrgdnAW8DD7r7YzG42s6lhtVuBnsAjZrbAzGZFFU+2MDO+et5I/n3q\n8Ty7ZDNX/eY1du+vizssEckQlmn3/y0vL/eKioq4w8gIj7++nm88spCxZcXce/UEirvlxh2SiMTE\nzOa5e/nB6umAcxf28ZNKuf2T43lz/U6u+NUctu+piTskEUlzSgpd3OQxR3Pnp05m2ebdzLh7Dlt2\nqy9fRFqnpJAF/uHY/tzzmVNYtXUPl9/5Cpt3VccdkoikKSWFLHHmyH7cd/UENu2s5vI7X2HDjn1x\nhyQiaUhJIYtMHN6X+z83ka27a/jEna+wZuveg68kIllFSSHLnDykNw9+4VR276/jsjv/zvLNVXGH\nJCJpREkhC51QVsxD15xGfQNcftcc3ly/M+6QRCRNKClkqQ8dXcgj155Gt9wkM+6aQ8WqbXGHJCJp\nQEkhiw3r14NHrj2NksJ8PvXr13hJ93wWyXpKClluYK9uPPTF0xjStzufvXcuM19bE3dIIhIjJQWh\npDCfh754GqeN6MuNj73BzU8s0T0ZRLKUkoIAUNwtl99cdQpXnT6Ue/72LlffO5fKKl39LJJtlBSk\nSU4ywU1Tj+eHF53AnJVbOf8/X+LpNzbGHZaIdCIlBfmAT04czB//6UxKe3Xjugfm8/WHFrBzX23c\nYYlIJ1BSkBaN7F/IY186na+eO5JZCzdw/k9f4i9vb447LBGJmJKCtCo3meDrHxnFH750OoUFOXz2\n3gq+cH8F67ZreAyRrkpJQQ5qbFkv/viVs/jXycfy8vItnPeTF7n9+RXU1DXEHZqIdDAlBWmXvJwE\n100awXPfOIdJo47i1tlLufDnf2WuroQW6VIiTQpmNtnMlprZCjO7sYXlZ5vZfDOrM7NLo4xFOsbA\nXt2441Mn86tPl7Nnfz2X3fEK33psETv26q5uIl1BZEnBzJLA7cAUYDQww8xGN6u2BrgKeDCqOCQa\n543uz5++fjbXnD2chyvWce5tL/L46+vJtHt+i8iBomwpTABWuPtKd68BZgLTUiu4+yp3XwTo4HQG\n6pGfw79dcByzrj+Dsj7d+dpDC5hxt0ZdFclkUSaFUmBtyvy6sEy6mOMHFvPYdafz/Y+PYemmKi78\n+ct8bebrOktJJANlREezmV1jZhVmVlFZqZE801EyYXzq1CG8+M0Pc+05I3jqzU38w20v8v+efkv9\nDSIZJMqksB4YlDJfFpYdMne/y93L3b28pKSkQ4KTaBQV5HLjlGN5/p8nceEJA7jzxZWcdcvz/PTZ\nZboqWiQDRJkU5gIjzWyYmeUB04FZET6fpJHSXt34yeUn8vRXz+KMY/rxs+eWc9Ytf+Enzy5j+x61\nHETSlUV5toiZXQD8J5AE7nH3H5jZzUCFu88ys1OAPwC9gWpgk7sf39Y2y8vLvaKiIrKYJRqLN+zk\nv55bzuzFm+mel+SKiYP5wlnDOaqoIO7QRLKCmc1z9/KD1su0UwiVFDLbss1V/PcL7zBr4QaSZlxW\nXsa154xgUJ/ucYcm0qUpKUhaW7N1L3e89A6PVqyj3p2p4wbypUkjGNm/MO7QRLokJQXJCJt3VXP3\nSyt54NU17Kut55xRJVx9xlDOHllCImFxhyfSZSgpSEbZtqeGB+as5v45q6ms2s+Ikh5cdcYwLj6p\nlB75OXGHJ5LxlBQkI9XUNfDUGxu552/vsmjdTrrnJZkyZgCXnFzKqcP6qvUgcpiUFCSjuTvz1+zg\nkYq1/HHRRqr211HaqxsXjy/lkvFlDO3XI+4QRTKKkoJ0Gftq6vnTkk38fv56Xl5eSYPDuEG9mDLm\naCYff7QShEg7KClIl7RpZzWPL1jPHxdt5I1w4L1R/Xty9sgSzhpVwsRhfSjITcYcpUj6UVKQLm/d\n9r088+YmXlhayWurtlFT10B+ToIJw/pw9sgSzhzZj1H9C0mqH0JESUGyy76aeua8u5W/LtvCS8sr\nWfHebgAK83MYP6Q35UN6c/LQ3pw4qBfd83Q2k2Sf9iYF/XdIl9AtL8mHP3QUH/7QUQCs37GPOe9s\npWL1duat3sZtzwaj6+YkjNEDixhbVszY0l6MHVTMMSU9yUlmxIDBIpFTS0Gyws69tcxfs52K1duY\nv3oHb67fSdX+OgAKchMcP7CYE0qLGT2wiNEDihjZvyf5OeqbkK5DLQWRFMXdc/nwsUfx4WODlkRD\ng7Nq6x4WrdvJonU7eWP9Dh6au5Z9tfVA0KI45qiejB5QxOiBRRw3IJj69MiL82WIRE4tBZFQfYOz\neuselmzcxZINu3hr4y6WbNzF5l37m+oMKC7guAFBa+K4MGEM6dNdF9VJ2lNLQeQQJRPG8JKeDC/p\nyYVjBzaVb929n7c2VrFk484wWVTx4rJK6huCH1QFuQmG9u3BsH7BNLRfD4aHf/v2yMNMCUMyh5KC\nyEH07ZnPmSPzOXNkv6ay6tp6Vry3myUbdrF0cxWrtuxh6aYqnl2ymbqG91vfhQU5TQliWL8DE0dR\nQW4cL0ekTUoKIoehIDfJmNKycYF1AAAMbElEQVRixpQWH1BeV9/Auu37eHfLngOmilXbmbVwA6lH\na/v1zAsSRN8eDCvpwbC+PTi6uID+RQWUFOaTqzOiJAZKCiIdKCeZYGjYEvhws2XVtfWs2baXlZVB\nolgVJoznl1byyLx1H9hWv555lBQW0L8on/6FQaLo1T2X3t3z6N0jl17d8+jTPY/e3fMoLMhRv4Z0\nCCUFkU5SkJtkVP9CRrVwI6Gq6lpWb93L5l3VbN61n/eqgr+V4d8lG3axdU9NUz9GcwmDwoJcCgty\ngr/5ORQW5NCzIKeprGd+DkWNZfm5Tcu75+XQLTdJt9wkBXkJ8pIJ9YNkMSUFkTRQWJDb4uGoVA0N\nTtX+OrbvqWH73hp27K1l+94atu+tZfueGqqqa6mqrqNqfx1V1bVs2lXN7sq6oKy6ltr69p1pmDCC\nJJGXQ7e8xPsJIzdJt7xkSgJpLE+Ql0ySl5MIpqSlPE6Se8B84oDHuckE+eF8brgsJ2FKSjGKNCmY\n2WTgZ0AS+JW7/6jZ8nzgfuBkYCtwubuvijImkUyVSBjF3XIp7pbLUA59ZNjq2nqqquvYHSaN3dV1\n7Kquo7q2nn219eyrCf5W19azt/Fx+Ldx+fY9NWxomm9gX00d+2rraaUBc1jMCJJHmCSSCSMnYSST\nRk4iSBrJhJGTNJKp8yl/c5LNyxMp2wjKc5OJD6yXur3c5ttPfrDe+2XNtpVsZ73wbzolwciSgpkl\ngduBjwDrgLlmNsvdl6RU+xyw3d2PMbPpwC3A5VHFJJLNCsJf+yWF+R2+7foGp6auIZjqwymcr61v\nYH/K46Y6dR+s19L6De7U1Tt1DcFU39BAXb1T3zTv1DUE6++tSS1veH95Sv26hgbq6w9ctyOT2uH4\nQGJrlrAaE9hXzx3Jx8YNPPgGjySWCLc9AVjh7isBzGwmMA1ITQrTgJvCx48CvzAz80y7ok4kyyUT\nFhxaysvMoUEaGpx6D5JEbX1Ds4TjYRJ5v7yu2Xx9CwmrttUEFpTX1reQwOoPrFfXLJZe3aM/jTnK\npFAKrE2ZXwdMbK2Ou9eZ2U6gL7AltZKZXQNcAzB48OCo4hWRLJVIGAmM3CRZfz+OjDgR2t3vcvdy\ndy8vKSmJOxwRkS4ryqSwHhiUMl8WlrVYx8xygGKCDmcREYlBlElhLjDSzIaZWR4wHZjVrM4s4DPh\n40uBv6g/QUQkPpH1KYR9BNcDswlOSb3H3Reb2c1AhbvPAn4N/NbMVgDbCBKHiIjEJNLrFNz9KeCp\nZmXfTXlcDVwWZQwiItJ+GdHRLCIinUNJQUREmigpiIhIk4y7HaeZVQKrD3P1fjS7MC6NpGtsiuvQ\nKK5Dl66xdbW4hrj7QS/0yrikcCTMrKI99yiNQ7rGprgOjeI6dOkaW7bGpcNHIiLSRElBRESaZFtS\nuCvuANqQrrEprkOjuA5dusaWlXFlVZ+CiIi0LdtaCiIi0gYlBRERaZI1ScHMJpvZUjNbYWY3xhjH\nIDN73syWmNliM/tqWH6Tma03swXhdEEMsa0yszfC568Iy/qY2bNmtjz827uTY/pQyj5ZYGa7zOxr\nce0vM7vHzN4zszdTylrcRxb4r/Azt8jMxndyXLea2dvhc//BzHqF5UPNbF/Kvrujk+Nq9b0zs2+F\n+2upmZ0fVVxtxPZQSlyrzGxBWN4p+6yN74fO+4y5e5efCEZpfQcYDuQBC4HRMcUyABgfPi4ElgGj\nCW5L+s8x76dVQL9mZT8Gbgwf3wjcEvP7uAkYEtf+As4GxgNvHmwfARcATwMGnAq82slxfRTICR/f\nkhLX0NR6MeyvFt+78P9gIZAPDAv/Z5OdGVuz5bcB3+3MfdbG90OnfcaypaXQdL9od68BGu8X3enc\nfaO7zw8fVwFvEdyWNF1NA+4LH98HfDzGWM4F3nH3w72i/Yi5+0sEw7ynam0fTQPu98AcoJeZDeis\nuNz9T+5eF87OIbjRVadqZX+1Zhow0933u/u7wAqC/91Oj83MDPgE8Luonr+VmFr7fui0z1i2JIWW\n7hcd+xexmQ0FTgJeDYuuD5uA93T2YZqQA38ys3kW3BcboL+7bwwfbwL6xxBXo+kc+E8a9/5q1No+\nSqfP3WcJflE2GmZmr5vZi2Z2VgzxtPTepdP+OgvY7O7LU8o6dZ81+37otM9YtiSFtGNmPYHfA19z\n913AfwMjgBOBjQRN1852pruPB6YAXzazs1MXetBejeUcZgvu3jcVeCQsSof99QFx7qPWmNm3gTrg\ngbBoIzDY3U8CbgAeNLOiTgwpLd+7ZmZw4A+QTt1nLXw/NIn6M5YtSaE994vuNGaWS/CGP+DujwG4\n+2Z3r3f3BuBuImw2t8bd14d/3wP+EMawubE5Gv59r7PjCk0B5rv75jDG2PdXitb2UeyfOzO7CrgQ\nuCL8MiE8PLM1fDyP4Nj9qM6KqY33Lvb9BU33i78YeKixrDP3WUvfD3TiZyxbkkJ77hfdKcJjlb8G\n3nL3n6SUpx4HvAh4s/m6EcfVw8wKGx8TdFK+yYH30f4M8L+dGVeKA365xb2/mmltH80CPh2eIXIq\nsDPlEEDkzGwy8E1gqrvvTSkvMbNk+Hg4MBJY2YlxtfbezQKmm1m+mQ0L43qts+JKcR7wtruvayzo\nrH3W2vcDnfkZi7o3PV0mgl76ZQQZ/tsxxnEmQdNvEbAgnC4Afgu8EZbPAgZ0clzDCc78WAgsbtxH\nQF/gOWA58GegTwz7rAewFShOKYtlfxEkpo1ALcHx28+1to8Izgi5PfzMvQGUd3JcKwiONzd+zu4I\n614SvscLgPnAxzo5rlbfO+Db4f5aCkzp7PcyLL8XuLZZ3U7ZZ218P3TaZ0zDXIiISJNsOXwkIiLt\noKQgIiJNlBRERKSJkoKIiDRRUhARkSZKCpI2zOzv4d+hZvbJDt72v7X0XFExs4+b2Xcj2va/HbzW\nIW/zBDO7t6O3K5lHp6RK2jGzSQSjaF54COvk+PuDv7W0fLe79+yI+NoZz98JLhrbcoTb+cDriuq1\nmNmfgc+6+5qO3rZkDrUUJG2Y2e7w4Y+As8Jx679uZkkL7g0wNxxE7Yth/Ulm9lczmwUsCcseDwf0\nW9w4qJ+Z/QjoFm7vgdTnCq8EvdXM3rTgXhKXp2z7BTN71IJ7EjwQXm2Kmf3IgvHuF5nZf7TwOkYB\n+xsTgpnda2Z3mFmFmS0zswvD8na/rpRtt/RarjSz18KyO1OuvN1tZj8ws4VmNsfM+ofll4Wvd6GZ\nvZSy+ScIrvaXbBblFYOaNB3KBOwO/04Cnkwpvwb4Tvg4H6ggGG9/ErAHGJZSt/FKz24Ewyf0Td12\nC891CfAswb0a+gNrCMa0nwTsJBhLJgG8QnC1aV+Cq20bW9m9WngdVwO3pczfCzwTbmckwdWzBYfy\nulqKPXx8HMGXeW44/0vg0+FjJ7zylmA8/sbnegMobR4/cAbwRNyfA03xTjntTR4iMfooMNbMLg3n\niwm+XGuA1zwYe7/RV8zsovDxoLDe1ja2fSbwO3evJxh07EXgFGBXuO11ABbcgWsowX0JqoFfm9mT\nwJMtbHMAUNms7GEPBoBbbmYrgWMP8XW15lzgZGBu2JDpxvuDpdWkxDcP+Ej4+G/AvWb2MPDY+5vi\nPWBgO55TujAlBckEBvyTu88+oDDoe9jTbP484DR332tmLxD8Ij9c+1Me1xPcxazOzCYQfBlfClwP\n/EOz9fYRfMGnat5557TzdR2EAfe5+7daWFbr7o3PW0/4/+7u15rZROAfgXlmdrIHI4AWhLFLFlOf\ngqSjKoJbETaaDVxnwZDCmNmocCTX5oqB7WFCOJbg9oSNahvXb+avwOXh8f0Sgls0tjoypwXj3Be7\n+1PA14FxLVR7CzimWdllZpYwsxEEgw8uPYTX1Vzqa3kOuNTMjgq30cfMhrS1spmNcPdX3f27BC2a\nxqGXRxHvaLOSBtRSkHS0CKg3s4UEx+N/RnDoZn7Y2VtJy7cFfQa41szeIvjSnZOy7C5gkZnNd/cr\nUsr/AJxGMDqsA990901hUmlJIfC/ZlZA8Cv9hhbqvATcZmaW8kt9DUGyKSIYgbPazH7VztfV3AGv\nxcy+Q3DHvATBiJ9fBtq6ZemtZjYyjP+58LUDfBj4YzueX7ownZIqEgEz+xlBp+2fw/P/n3T3R2MO\nq1Vmlg+8SHD3vVZP7ZWuT4ePRKLxQ6B73EEcgsHAjUoIopaCiIg0UUtBRESaKCmIiEgTJQUREWmi\npCAiIk2UFEREpMn/B/HO6bji7bRDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy -->0.9999999999999998\n",
            "Testing Accuracy --->0.7000000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C-VWC02YXK6",
        "colab_type": "code",
        "outputId": "fe1706ad-beb0-4d75-fda5-414f26db812c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def predict(X,Y,parameters):\n",
        "  AL, caches = L_model_forward(parameters=parameters,X=X)\n",
        "  yhat= AL > 0.5\n",
        "  acc = np.sum((yhat == Y)/X.shape[1])\n",
        "  return AL,yhat,acc\n",
        "\n",
        "AL,yhat,acc = predict(test_x,test_y,parameters)\n",
        "acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7000000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxD5z61Vt5iR",
        "colab_type": "code",
        "outputId": "69747544-7d4a-45b7-d12f-435457c6cfaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print(\"Train Accuracy -->{}\".format(predict(train_x,train_y,parameters)))\n",
        "# print(\"Testing Accuracy --->{}\".format(predict(test_x,test_y,parameters)))\n",
        "# print(test_y)\n",
        "# print(yhat.astype(int))\n",
        "yhat = yhat.astype(int)\n",
        "print(yhat[0][6])\n",
        "print(AL[0][6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0.2461497748298468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dihUY_H3d7nS",
        "colab_type": "code",
        "outputId": "252e05a1-4f93-4fcb-ce0e-92758c9d2984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "# Example of a picture\n",
        "index = 1\n",
        "plt.imshow(test_x_orig[index])\n",
        "print (\"y = \" + str(test_y[0,index]) + \". It's a \" + classes_test[test_y[0,index]].decode(\"utf-8\") +  \" picture.\")\n",
        "print (\"yhat = \" + str(yhat[0,index]) + \". Predicted \" + classes_test[yhat[0,index]].decode(\"utf-8\") +  \" picture.\")\n",
        "print(\"Al --->\",AL[0][index])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = 1. It's a cat picture.\n",
            "yhat = 0. Predicted non-cat picture.\n",
            "Al ---> 0.24621566828423902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLCBi4AcCC4j/Dxf4+yS35dD2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGBM3pL-xj5S",
        "colab_type": "text"
      },
      "source": [
        "End Deep Neural Network"
      ]
    }
  ]
}